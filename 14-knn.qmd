---
title: "Section 14: k-nearest-neighbors"
author: "Matt Higham"
format: 
  html:
    embed-resources: true
---

```{r}
set.seed(1119)
library(tidyverse)
library(here) 

pokemon <- read_csv(here("data", "pokemon_full.csv")) |>
  filter(Type %in% c("Steel", "Dark", "Fire", "Ice")) |>
  mutate(across(where(is.numeric), ~ (.x - min(.x)) /
                                 (max(.x) - min(.x)))) 
## performs scaling on all of the quantitative variables
## in the data frame
## where(is.numeric) says to only run the formula if
## the variable class is numeric

train_sample <- pokemon |>
  slice_sample(n = 75)
## slice_sample(n = 75) randomly selects 75 rows to be in
## training sample

test_sample <- anti_join(pokemon, train_sample)
## anti_join() filters pokemon to remove any observations
## that are in the train_sample
```

```{r}
## install.packages("GGally")
library(GGally)
ggpairs(data = train_sample, columns = c("SpAtk", "height", "weight", "Type"))
## if the boxplots in the last column have very different centers
## and not a lot of overlap, then that hints that the predictor
## would be useful for classifying into type

## the diagonal gives density plots, which are like smoothed
## histograms
```

```{r}
## install.packages("class")
library(class)

## create a data frame that only has the predictors
## that we will use
train_small <- train_sample |> select(HP, Attack, Defense, Speed, SpAtk, weight)
test_small <- test_sample |> select(HP, Attack, Defense, Speed, SpAtk, weight)

## put our response variable into a vector
train_cat <- train_sample$Type
test_cat <- test_sample$Type

knn_mod <- knn(train = train_small, test = test_small,
               cl = train_cat, k = 19)
knn_mod
## returns a vector of classifications in the test sample
```

```{r}
table(knn_mod, test_cat) 
## knn_mod is the vector of classifications (predictions of Type)
## test_cat is the actual levels of Type
## rows are predictions, columns are the actual values
```

Exercise 2. 11 Pokemon were correctly predicted to be Fire.

Exercise 3. 3 Pokemon were incorrectly predicted to be Dark when thgey were actually Fire pokemon.

Exercise 5.

```{r}
test_sample
(1 + 11 + 1 + 3) / 45

tab <- table(knn_mod, test_cat) 
sum(diag(tab)) / sum(tab)
## diag() only gives the values on the diagonal which
## correspond to the correct classifications
```

Exercise 7.

```{r}
## figure out the most common type in train_sample
train_sample |> group_by(Type) |>
  summarise(n_type = n())
## Fire is most common

## find the proportion of pokemon in test_sample
## that match the most common type in train_sample
test_sample |> mutate(is_fire = if_else(Type == "Fire",
                                        true = 1,
                                        false = 0)) |>
  relocate(is_fire) |>
  summarise(prop_fire = mean(is_fire))
```

Exercise 8. A larger k incorporates more observations in the training sample when classifying a case in the test sample. But, too large of a k would result in incorporating observations in the training sample that aren't that similar to the test case observation.

aside:
low k: low bias, high variance
high k: high bias, low variance



## Class Exercises

```{r}
library(tidyverse)

pokemon <- read_csv(here::here("data/pokemon_full.csv")) 
set.seed(1119)

## scale the quantitative predictors
pokemon_scaled <- pokemon |>
  mutate(across(where(is.numeric), ~ (.x - min(.x)) /
                  (max(.x) - min(.x))))

train_sample <- pokemon_scaled |>
  slice_sample(n = 550)
test_sample <- anti_join(pokemon_scaled, train_sample)

library(class)

train_pokemon <- train_sample |> select(HP, Attack, Defense, Speed,
                                        SpAtk, SpDef, height, weight)
test_pokemon <- test_sample |> select(HP, Attack, Defense, Speed,
                                      SpAtk, SpDef, height, weight)

## put our response variable into a vector
train_cat <- train_sample$Type
test_cat <- test_sample$Type

knn_mod <- knn(train = train_pokemon, test = test_pokemon,
               cl = train_cat, k = 19)
knn_mod

tab <- table(knn_mod, test_cat)
sum(diag(tab)) / sum(tab)
## classification rate is lower perhaps because there
## are more types to classify into
```

```{r}
get_class_rate <- function(k_val) {
  knn_mod <- knn(train = train_pokemon, test = test_pokemon,
                 cl = train_cat, k = k_val)
  knn_mod
  
  tab <- table(knn_mod, test_cat)
  class_rate <- sum(diag(tab)) / sum(tab)
  
  return(class_rate)
}
get_class_rate(k_val = 21)

## b
k_vec <- 1:60
k_vec

## map() is from purrr
class_rates <- map(k_vec, get_class_rate) |> unlist()
class_df <- tibble(k_vec, class_rates)
class_df

## make a line plot that looks at the classification rate
## for different k values

train_sample |> group_by(Type) |>
  summarise(n_type = n()) |>
  arrange(desc(n_type))
## Water is most common

## find the proportion of pokemon in test_sample
## that match the most common type in train_sample
test_sample |> mutate(is_water = if_else(Type == "Water",
                                        true = 1,
                                        false = 0)) |>
  relocate(is_water) |>
  summarise(prop_water = mean(is_water))


ggplot(data = class_df, aes(x = k_vec, y = class_rates)) +
  geom_line() +
  geom_hline(yintercept = 0.14, linetype = 2) +
  theme_minimal()
## add baseline classification rate for this example
## as a horizontal line
```